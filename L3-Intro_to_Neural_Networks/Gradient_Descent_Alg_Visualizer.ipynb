{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Gradient Descent Algorithm + Interactive Visualizer\n",
    "\n",
    "In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset. Moreover, an interactive Visualizer will be implemented in order to see the the effect on the Cross-Enropy of each iteration. First, we'll start with some functions that will help us plot and visualize the data.\n",
    "\n",
    "Please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# Plotting function for both points and lines\n",
    "def plot_routine(fig, data, errors, m, b):\n",
    "    \n",
    "    data['errors'] = errors\n",
    "     \n",
    "    x1 = np.arange(-1, 3, 1) \n",
    "    x2 = m*x1+b            \n",
    "    \n",
    "    data = [go.Scatter(mode='markers', visible=False, marker_color=data[\"colors\"], marker=dict(size=data['errors'], \n",
    "                        sizeref = 0.025, line=dict(color='Black', width=1)), x=data[\"x1\"], y=data[\"x2\"],\n",
    "                        hoverinfo=\"text\", hovertemplate= \"Cross-Entropy: %{marker.size:.3f}\" + \"<extra></extra>\"),\n",
    "            go.Scatter(mode=\"lines\",visible=False, x = x1, y = x2, line=dict(color=\"black\"))]\n",
    "    \n",
    "    fig.add_traces(data)\n",
    "\n",
    "# Plotting function for the slider\n",
    "def plot_slider(fig):\n",
    "    \n",
    "    steps= []\n",
    "    \n",
    "    # Define the steps dict, for each step of the slider we want just two dataset to be visible:\n",
    "    # one boundary_line and the related set of points (with the related error)\n",
    "    for i in range(int(len(fig.data)/2)):\n",
    "        step = dict(method=\"restyle\",args=[\"visible\", [False] * len(fig.data)])\n",
    "        \n",
    "        step[\"args\"][1][2*i] = True  # Toggle i'th trace to \"visible\"\n",
    "        step[\"args\"][1][2*i+1] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "    \n",
    "    active_level = 0    # Active Level of dataset when the plot is generated\n",
    "    fig.data[active_level].visible = True\n",
    "    fig.data[active_level+1].visible = True\n",
    "    \n",
    "    sliders = [dict(\n",
    "    active= active_level,   # Slider starting level\n",
    "    currentvalue={\"prefix\": \"Iteration: \"},\n",
    "    pad={\"t\": 50}, \n",
    "    steps=steps)]\n",
    "    \n",
    "    # Updating the plot with the designed slider\n",
    "    fig.update_layout(sliders=sliders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions\n",
    "\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation (sigmoid) function, Continuous function \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Prediction function based on the sigmoid function (continuos output)\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "# Error function based on the Cross-Entropy for a 2 dimension set\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
    "\n",
    "# Perceptron weights/bias update, based on the Gradient Descent\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = y - output\n",
    "    weights += learnrate * d_error * x\n",
    "    bias += learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Algorithm\n",
    "def train(data, features, targets, epochs, learnrate, graph_lines=False):\n",
    "    \n",
    "    # Plot Init, layout settings\n",
    "    layout = go.Layout(\n",
    "    showlegend=False,\n",
    "    title = \"Gradient Descent Algorithm\",\n",
    "    template = pio.templates['plotly'],\n",
    "    xaxis = dict(range=[-0.01, 1.01]),\n",
    "    yaxis = dict(range=[-0.01, 1.01]))\n",
    "    \n",
    "    fig = go.Figure(layout=layout)\n",
    "    \n",
    "    #Arrays Init\n",
    "    errors = []\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "    \n",
    "    # Iterations\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Weights and Bias update based on the entire data set \n",
    "        for x, y in zip(features, targets):\n",
    "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
    "        \n",
    "        # Prediction function (outuput continuos value == propability to be classify right)\n",
    "        out = output_formula(features, weights, bias)\n",
    "        \n",
    "        # log-loss error on the entire training set (Continuos Error)\n",
    "        loss = np.mean(error_formula(targets, out))\n",
    "        errors.append(loss)\n",
    "        \n",
    "        # Printing Train log-loss error data\n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            if last_loss and last_loss < loss:\n",
    "                #If loss increase (every epochs/100 steps) we have an alert \n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            \n",
    "            # Accuracy on the entire training set (Discrete Error)\n",
    "            predictions = out > 0.5\n",
    "            accuracy = np.mean(predictions == targets)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "        \n",
    "        # Plot Boundary lines and points + Error as Size every epochs/100 steps\n",
    "        if graph_lines and e % (epochs / 100) == 0:\n",
    "            \n",
    "            output = output_formula(features, weights, bias)       \n",
    "            point_errors = error_formula(targets,output)\n",
    "            \n",
    "            plot_routine(fig, data, point_errors, -weights[0]/weights[1], -bias/weights[1])\n",
    "    \n",
    "    # Slider implementation\n",
    "    plot_slider(fig)\n",
    "    \n",
    "    #Plotting\n",
    "    plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading  and Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "data = pd.read_csv('DataSet1.csv', header=None)\n",
    "X = np.array(data[[0,1]])\n",
    "y = np.array(data[2])\n",
    "\n",
    "data.columns = [\"x1\",\"x2\",\"y\"]\n",
    "\n",
    "# Associate '0' and '1' with 'red' and 'blue'\n",
    "colors = []\n",
    "for i in range(len(data['y'])):\n",
    "    if data['y'][i] == 1:\n",
    "       colors.append('red')\n",
    "    else:\n",
    "       colors.append('blue') \n",
    "\n",
    "data['colors'] = colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Algorithm!\n",
    "\n",
    "In the graph that is outputed, we have a slider that gives the posibility to see each Iteration of the Algorithm. \n",
    "Moving the slider right, we can se how the boundary line moves closer to the ideal position. \n",
    "\n",
    "The size of each points decrease as the Cross-Entropy (now on CE) decrease. Remember that the CE represent the inverse of the propability of a point to be classified right. Bigger the CE, lower the probability that the point has been classify correctly.  \n",
    "\n",
    "Play with the paramters below (epochs and learnrate), to see how the result of the Algorithm changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Epoch 0 ==========\n",
      "Train loss:  0.7927919394151404\n",
      "Accuracy:  0.48\n",
      "\n",
      "========== Epoch 10 ==========\n",
      "Train loss:  0.6569602877606614\n",
      "Accuracy:  0.5\n",
      "\n",
      "========== Epoch 20 ==========\n",
      "Train loss:  0.5820698219229617\n",
      "Accuracy:  0.67\n",
      "\n",
      "========== Epoch 30 ==========\n",
      "Train loss:  0.5232551140606432\n",
      "Accuracy:  0.8\n",
      "\n",
      "========== Epoch 40 ==========\n",
      "Train loss:  0.4768761340345054\n",
      "Accuracy:  0.88\n",
      "\n",
      "========== Epoch 50 ==========\n",
      "Train loss:  0.4397318048929534\n",
      "Accuracy:  0.92\n",
      "\n",
      "========== Epoch 60 ==========\n",
      "Train loss:  0.4094966649977223\n",
      "Accuracy:  0.93\n",
      "\n",
      "========== Epoch 70 ==========\n",
      "Train loss:  0.384501319649033\n",
      "Accuracy:  0.92\n",
      "\n",
      "========== Epoch 80 ==========\n",
      "Train loss:  0.36354165945669936\n",
      "Accuracy:  0.93\n",
      "\n",
      "========== Epoch 90 ==========\n",
      "Train loss:  0.34573937809486516\n",
      "Accuracy:  0.93\n"
     ]
    }
   ],
   "source": [
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed()\n",
    "\n",
    "# Input Parameter, Epochs represent the number of updates/iterations, Learning Rate the step\n",
    "# between consecutive updates \n",
    "epochs = 100   # MIN = 100\n",
    "learnrate = 0.01\n",
    "\n",
    "# Main Program execution    \n",
    "train(data, X, y, epochs, learnrate, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
